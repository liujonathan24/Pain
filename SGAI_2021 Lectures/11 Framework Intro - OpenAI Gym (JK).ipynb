{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym Intro\n",
    "\n",
    "OpenAI Gym is a standard python API for RL environments. It's primary use is to help baseline and measure the effectiveness of different RL algorithms. In this course, we will use it a bit differently. We will be building a custom environment and using well known RL algorithms to analyze the environment we modeled!\n",
    "\n",
    "Why do we use OpenAI Gym for this? It provides a lot of helpful tools for building out environments in a systematic way. It also includes a lot of build in things to help make the job faster and easier.\n",
    "\n",
    "In this notebook, we'll show some basic OpenAI Gym functionality and hit on the major elements of the API.\n",
    "\n",
    "Examples adapted from: \n",
    "- https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html\n",
    "- https://ai-mrkogao.github.io/reinforcement%20learning/openaigymtutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logic Flow\n",
    "In the example below, we can see the major elements coming together. \n",
    "1. First we are going to instantiate our environment (here we will use an out of the box environment called Taxi).\n",
    "2. We reset our environment and obtain the initial state\n",
    "3. We begin taking steps (in this case a maximum of 1000 steps)\n",
    "4. We render the environment so we can see the initial state\n",
    "5. Our agent (human or machine) makes actions that we capture and pass into the step function\n",
    "6. Our step function will execute the game logic, consuming actions and returning a new state (observation), reward, done, and info\n",
    "7. We check if we are done, stop if so, continue if not\n",
    "8. This ends the \"episode\". For training, we would want to run many episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    # env.render()  # usually we render but we'll skip in jupyter\n",
    "    # your agent here (this takes random actions)\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        # env.render()  # usually we render but we'll skip in jupyter\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "There are 4 major space datatypes that OpenAI Gym provides. There are:\n",
    "1. Discrete\n",
    "2. MultiDiscrete\n",
    "3. Box\n",
    "4. Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Space\n",
    "A fixed and finite set of points, which map directly to actions or states. This is mostly used for action spaces as state spaces tend to be much more rich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_space = gym.spaces.Discrete(10)\n",
    "discrete_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiDiscrete Space\n",
    "Contains k-dimensions, each one made up a discrete space. This is mostly used to capture state spaces and one hot encoding of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multidiscrete_space = gym.spaces.MultiDiscrete([5, 2, 2])\n",
    "multidiscrete_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Space\n",
    "This is similar to the MultiDiscrete Space except that it allows continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59720695, -1.6172327 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "box_space = gym.spaces.Box(np.array((-1.0, -2.0)), np.array((1.0, 2.0)))\n",
    "box_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuple Space\n",
    "While not used frequently, it can be very helpful. The tuple space allows you to combine different simple states into a single space. In the example below we combine two different discrete spaces into a single tuple space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_1 = gym.spaces.Discrete(2)\n",
    "space_2 = gym.spaces.Discrete(3)\n",
    "tuple_space = gym.spaces.Tuple((space_1, space_2))\n",
    "tuple_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Environments\n",
    "Finally we will show a bare bones example of an example environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Custom Environment\n",
    "This shows the basic environment structure with the mandatory functions being overridden. In this example, we show a discrete action space with a box state space used to capture RBG channel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        N_DISCRETE_ACTIONS = 5\n",
    "        HEIGHT = 100\n",
    "        WIDTH = 100\n",
    "        N_CHANNELS = 3\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        # Example for using image as input:\n",
    "        self.observation_space = spaces.Box(low=0, \n",
    "                                            high=255,\n",
    "                                            shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "                                            dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation = 'observation'\n",
    "        reward = 1\n",
    "        done = false\n",
    "        info = 'test'\n",
    "        return observation, reward, done, info\n",
    "  \n",
    "    def reset(self):\n",
    "        pass\n",
    "        observaton = 'observation'\n",
    "        return observation  # reward, done, info can't be included\n",
    "  \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close (self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Custom Environment\n",
    "We can then use this environment like the build in OpenAI Gym environments. While we usually set up human play and RL training / playback, this example just shows how we can interrogate the action and state spaces of a given OpenAI Gym environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sample: 3\n",
      "State Sample: [[[ 35 131 160]\n",
      "  [217 183 230]\n",
      "  [173 160 135]\n",
      "  ...\n",
      "  [163 218  93]\n",
      "  [177  70 191]\n",
      "  [162  92 140]]\n",
      "\n",
      " [[204 239  73]\n",
      "  [234  15 247]\n",
      "  [130 166  96]\n",
      "  ...\n",
      "  [ 35  55  67]\n",
      "  [173 163  96]\n",
      "  [ 95 190 226]]\n",
      "\n",
      " [[ 55 135  79]\n",
      "  [ 17 185  16]\n",
      "  [243 229 156]\n",
      "  ...\n",
      "  [151  25  74]\n",
      "  [ 89 195   8]\n",
      "  [ 50  76 135]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[145  78 245]\n",
      "  [156 151   2]\n",
      "  [ 78  12 140]\n",
      "  ...\n",
      "  [229 141 113]\n",
      "  [254  80  49]\n",
      "  [196 143 209]]\n",
      "\n",
      " [[231 243 231]\n",
      "  [ 13  48  30]\n",
      "  [ 37 145 134]\n",
      "  ...\n",
      "  [141 113 119]\n",
      "  [240  75 244]\n",
      "  [ 19 146  56]]\n",
      "\n",
      " [[184 214 151]\n",
      "  [ 70 215 103]\n",
      "  [152 193 194]\n",
      "  ...\n",
      "  [ 85 200  42]\n",
      "  [152  47 176]\n",
      "  [208  96 162]]]\n"
     ]
    }
   ],
   "source": [
    "env = CustomEnv()\n",
    "sample_action = env.action_space.sample()\n",
    "sample_state = env.observation_space.sample()\n",
    "print('Action Sample: {}'.format(sample_action))\n",
    "print('State Sample: {}'.format(sample_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
